# 第 8 章 多项式回归与模型泛化

- 线性回归的前提假设是存在线性关系
  - 但是数据之间的关系不一定是线性的

- 比如x^2的二次关系，虽然不是线性回归，但是相当于多加入特征，这个特征是原来特征的多项式项

## 过拟合和欠拟合(Overfitting, Underfitting)

- 不能随意使用多项式回归，容易出现问题

- 多项式回归使用R^2，或者使用MSE都是可以的

- 随意增加次数进行拟合，能尽可能的是所有的点存在于拟合曲线上
  - 但是该拟合曲线并不是想要的结果
  - 过拟合
    - 算法所训练的模型过多地表达了数据间的噪音关系

- 使用非常少的次数进行拟合，太过于简单
  - 欠拟合
    - 算法所训练的模型不能完整表述数据关系

## 模型的泛化能力

- 如果对一个模型过拟合，那么拟合曲线的形状可能非常奇怪
  - 出现新的数据点的时候，预测的值也可能非常奇怪，于原来的数据偏离较大
  - 此时预测能力太差，也就是**模型的泛化能力弱**
  - 面对新的数据预测能力太差

- 真正需要的是模型的泛化能力强才行

- 所以分离训练数据和测试数据的意义也就在此
  - 如果面对新的数据也能很好的预测，才能说明模型的泛化能力强

- 模型准确率vs模型复杂程度曲线
  - 对于训练数据集来说，模型复杂度越高，准确率越高
  - 对于测试数据集来说，存在一个极大值，模型复杂度比极大值还大的时候准确率降低
  - 每一个模型的复杂程度定义不一样

- 学习曲线
  - 随着训练样本的逐渐增多，算法训练出的模型的表现能力

- 使用train-test-split判断模型的泛化能力也有一个问题
  - 针对特定测试数据集产生过拟合问题
  - 因为测试数据集也是已知的，所以有可能是在对测试数据集调参

- 所以需要将数据分成三部分
  - 训练数据集训练模型，验证数据集达到最优(调整超参数使用的数据集)，测试数据集作为衡量最终模型性能的数据集
  - 训练，验证都参与模型的创建，但是测试数据是完全不可知的，不参与模型的创建

- 但是还是有问题
  - 因为是随机分割的数据集，所以如果验证数据集有极端数据可能会导致模型不准确
  - 通过交叉验证Cross Validation进行解决

## 交叉验证

- 比较正规，一般使用该方法进行调参

- 先将数据集分成训练数据集和测试数据集
- 然后将训练数据集分成k份A,B,C。然后将其中k-1个进行训练，第k个进行验证
  - 最终得到的k个模型的均值作为结果调参

- k-folds交叉验证
  - 把训练数据集分成k份，成为k-folds cross validation
- 缺点，每次训练k个模型，相当于整体性能满了k倍
  
- 极端情况下k-folds法可以变成留一法LOO-CV(Leave-One-Out Cross Validation)
  - 如果训练数据集有m个样本，就分成m份
  - 每次训练m-1个样本，用1个样本进行验证
  - 完全不受随机的影响，最接近模型真正的性能指标
  - 缺点：计算量巨大

## 偏差方差权衡(Bias Variance Trade off)

- 偏差：偏离于真值的程度
- 方差：偏离于均值的程度

- 想要预测的结果想象为真值的时候，模型误差来源于
  - 模型误差=偏差+方差+不可避免的误差
    - 不可避免的误差：比如数据本身问题

- 导致偏差的主要原因：对问题本身的假设不正确
  - 比如：非线性数据使用线性回归
  - 欠拟合
  - 特征与预测目标高度不想关

- 方差：数据的一点点扰动都会较大地影响模型
  - 通常原因：使用的模型太复杂，比如告诫多项式回归
  - 过拟合

- 有一些算法天生就是高方差算法：kNN
  - 非参数学习通常都是高方差算法，因为不对数据进行任何假设
- 有一些算法天生是高偏差算法：线性回归
  - 参数学习通常都是高偏差算法，因为对数据具有极强的假设
  - 将数据归结成数学模型

- 大多数算法具有相应的参数可以调整偏差和方差
  - kNN中的k，k越大模型越简单，偏差越大，方差越小
  - 线性回归中的多项式回归的degree。阶数越大，模型越复杂，方差越大，偏差越小

- 偏差和方差通常是矛盾的。降低一方，提高另一方
  - 要找到平衡

- 方差太高：泛化能力太差
- 偏差太高：模型偏离原问题

- 机器学习算法层面的主要挑战，来自于方差，也就是泛化能力太差，过拟合
  - 解决高方差的通常手段：
    - 降低模型复杂度
    - 减少数据维度：降噪
    - 增加样本数
    - 使用验证集
    - **模型正则化**

## 模型正则化Regularization

- 模型正则化：限制参数的大小

- 岭回归Ridge Regression
  - 目标：使J=MSE+α/2*θ平方和最小

- LASSO Regression(Least Absolute Shrinkage and Selection Operator Regression)
  - 目标：使J=MSE+α*|θ|绝对值和最小
  - Ridge的时候即使α取值非常大的时候，得到的还是一根曲线，实际上岭回归很难得到一个直线
  - LASSO的α即使取值比较小的时候，也可能得到一根近乎直线的曲线
    - LASSO趋向于使得一部分theta值变为0.所以可作为特征选择用(Selection Operator)

- 岭回归的后半部分是平方和，求梯度的时候可以发现每一个θ都还是保留值的
  - 在走向极值的路径中，每一个θ都会保留着值。走的是最小梯度方向
  - 岭回归更像爬山，不断翻山越岭
  - 准确度的话岭回归更好
- 而LASSO的绝对值部分不可导，所以需要使用分类函数
  - 分类函数性质导致走向极值的路径中，会先将某一个θ变为0，以此类推，是不断将θ变为0的路径
  - LASSO的性质导致可能会把有用的特征参数变为0
  - 但是特征数太多的时候还是很有用的

- Ridge -- MSE -- 欧拉距离
  - 平方和
- LASSO -- MAE -- 曼哈顿距离
  - 绝对值

## 弹性网

- 明可夫斯基距离的p
  - L1正则，L2正则
  - Lp范数
  - 很少使用p>2的正则项

- Ridge：L2正则项
- LASSO：L1正则项

- L0正则项：非0θ元素个数越少越好
  - 很少使用。因为L0正则的优化是一个NP难问题。因为L0正则项是一个离散项，需要穷举。实际用L1取代(LASSO)

- 弹性网：Elastic Net
  - 在MSE后添加L1正则项和L2正则项
  - 需要再引入超参数r，表达L1正则项和L2正则项的比例
  - 同时结合了Ridge和LASSO的优势
  
- 实际尝试中应该先尝试岭回归(计算能力够强的话)
- 如果特征量太大的话优先选择弹性网
  - 因为只是用LASSO的话，因为LASSO太急于将θ变为0，容易发生错误