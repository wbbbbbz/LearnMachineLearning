# 第 5 章 PCA与梯度上升法

- 不是一个机器学习算法，是一种基于搜索的最优化方法
- 作用：最小化一个损失函数
  - 梯度上升法：最大化一个效用函数

## 梯度下降法
- 损失函数J和参数theta之间的函数
  - 当导数dJ/dtheta不为0的时候，说明不是极值(最小值)
  - 导数：切线的斜率，J增大的方向
    - 希望是J减小的方向移动，所以向导数的负方向移动
    - 移动步长为η，提前取好这个值
  - 如果当导数不为0，就让theta移动-ηdJ/dtheta值
    - 直到取到损失函数J取最小值
    - 因为过程是J的值逐步下降，所以称作梯度下降法
  - 一维函数是导数，多维函数就要对各个方向的分量进行求导，就是梯度

- η成为学习率(learning rate)
  - 超参数
  - 取值影响获得最优解的速度
  - 取值不合适，甚至得不到最优解
    - 太小减慢收敛学习速度
    - 太大甚至导致不收敛
  - 梯度下降法的一个超参数
  - η一开始设置为0.01会比较好

- 并不是所有函数只有唯一的极值点
  - 局部最优解，全局最优解
  - 解决方案：多次运行，随机化初始点，能否找到更好的解
    - 梯度下降法的初始点也是一个超参数

- 线性回归法的损失函数具有唯一的最优解

### 线性回归中的梯度下降法

- theta是向量，包括常数项

- 梯度代表方向，对应J增大最快的方向

- 如果直接求导数的话会发现样本数量越大，梯度就越大，这样不合理
  - 所以会除以样本数量
  - 实际上等同于损失函数发生了改变，损失函数也除以样本数量。
  - 也就是J(theta) = MSE(y, yhat)

- 可以进行矩阵运算
  - 矩阵运算时候关注维度变化！



