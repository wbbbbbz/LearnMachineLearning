# 第 6 章 梯度下降法

- 不是一个机器学习算法，是一种基于搜索的最优化方法
- 作用：最小化一个损失函数
  - 梯度上升法：最大化一个效用函数

## 梯度下降法
- 损失函数J和参数θ之间的函数
  - 当导数dJ/dθ不为0的时候，说明不是极值(最小值)
  - 导数：切线的斜率，J增大的方向
    - 希望是J减小的方向移动，所以向导数的负方向移动
    - 移动步长为η，提前取好这个值
  - 如果当导数不为0，就让θ移动-ηdJ/dθ值
    - 直到取到损失函数J取最小值
    - 因为过程是J的值逐步下降，所以称作梯度下降法
  - 一维函数是导数，多维函数就要对各个方向的分量进行求导，就是梯度

- η成为学习率(learning rate)
  - 超参数
  - 取值影响获得最优解的速度
  - 取值不合适，甚至得不到最优解
    - 太小减慢收敛学习速度
    - 太大甚至导致不收敛
  - 梯度下降法的一个超参数
  - η一开始设置为0.01会比较好

- 并不是所有函数只有唯一的极值点
  - 局部最优解，全局最优解
  - 解决方案：多次运行，随机化初始点，能否找到更好的解
    - 梯度下降法的初始点也是一个超参数

- 线性回归法的损失函数具有唯一的最优解

### 线性回归中的梯度下降法

- θ是向量，包括常数项

- 梯度代表方向，对应J增大最快的方向

- 如果直接求导数的话会发现样本数量越大，梯度就越大，这样不合理
  - 所以会除以样本数量
  - 实际上等同于损失函数发生了改变，损失函数也除以样本数量。
  - 也就是J(θ) = MSE(y, yhat)

- 可以进行矩阵运算
  - 矩阵运算时候关注维度变化！

- 批量梯度下降法(Batch Gradient Descent)
  - 每一次计算都要使用所有的数据进行计算
  - 改进：随机梯度下降法

## 数据归一化

- 使用梯度下降法前最好进行数据归一化
  - 因为有eta变量，如果最终数值不在一个维度上会影响梯度的结果
  - eta乘上梯度的值就会无法控制


## 梯度下降法的优势

- 因为线性回归法的计算复杂度太高，所以数据特征多的时候梯度下降法有一定的优势
- 但是梯度下降法要对所有数据都进行运算，所以数据量大的情况还可以进行优化


## 随机梯度下降法(Stochastic Gradient Descent)

- 因为批量梯度下降法需要对所有的数据进行计算，当数据量大的时候性能低下
- 所以每一次计算的时候可以随机只计算一个数据，以该计算出来的向量当作方向
  - 此时不需要除以个数m
  - 计算方式和批量梯度下降法是一样的，只不过样本量不一样
- 但是因为每一次搜索方向不保证是最小的方向，也不保证一定是缩小的方向
  - 有一定的不可预知性
- 但是实验结论揭露随机梯度下降法基本能到达最小值附近
  - 虽然不一定能保证来到最小值
  - 但是数据量大的时候可以使用精度换取时间
- 随机梯度下降法中学习率的取值是非常重要的
  - 如果学习率是固定的话，有可能来到最小值附近之后还有可能因为随机问题而跳出最小值空间
- 需要设计eta使重复次数越大，eta越小，也就是学习率随着循环次数增加逐渐递减
  - 比如倒数，但是重复次数小的时候eta变化太快
  - 一般在分母中加上一个b，分子也可以取a，更加灵活
  - a和b也是超参数
  - 经验上a=5,b=50
- eta的设计使模拟搜索中的重要思想，**模拟退火**
  - 模拟自然界中打造钢铁，火炼的温度使从高到低逐渐减低
    - 也就是随时间变化而变化
    - 所以a和b使t0和t1，时间关系的参数

- 想法：
  - 使用sgd求出比较好的θ，代入BGD中呢？

## 关于梯度的调试

- 梯度的计算是复杂的(特别是非线性的时候)，如何发现梯度的计算是错误的？

- 求某一点的导数的时候，可以求该点的左右某两点的连线的斜率，近似为导数(左右某两点尽可能接近即可)
  - dJ/dθ = (J(θ+ε) - J(θ-ε))/2ε
  - 偏导数也是一样，对θ0求偏导的时候，只改变对应的θ0即可

- 这种近似法的计算量太高，J的复杂度太高的时候计算量太大
  - 所以用于使用小数据进行调试，看看对不对
  - 先进行调试之后，再通过结果求出数学解，再进行随机梯度下降法
  - 这种调试方法与J的表达式无关，所以可以作为通用工具使用


## 总结

- 批量梯度下降法，随机梯度下降法，小批量梯度下降法(Mini-Batch Gradient Descent)
  - 小批量梯度下降法综合上面两个梯度法
  - 每一次不看所有的样本，也不只看一个样本，而是看一个小批量，比如10个，20个的样本的梯度，然后进行计算即可
  - 又多一个超参数k(小批量的值)

- 随机的意义
  - 跳出局部最优解
  - 更快的运行速度
  - 机器学习领域很多算法都要使用随机的特点: 随机搜索；随机森林

- 梯度上升法
  - 求的是函数增加的方向
  - 最大化一个效用函数





