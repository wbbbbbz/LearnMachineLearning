# 第 5 章 线性回归法

- 解决回归问题
  - 思想简单，实现容易
  - 许多强大的非线性模型的基础
  - 结果具有很好的可解释性
  - 蕴含机器学习中的很多重要思想

- 寻找一条直线，最大程度的拟合样本特征和样本输出标记之间的关系
  - 横轴是样本特征，纵轴是样本输出标记
  - 因为输出是数值
  - 样本特征只有一个，成为简单线性回归
  - y=ax+b

- 希望预测值和真值之间的差距尽量小
  - 差距的平方和尽可能小的a和b
  - 最小二乘法

- 一类机器学习算法的基本思路
  - 找到参数使得预测值和真值之间的差距尽量小
  - 预测值和真值之间的差距可以使用损失函数(loss function)
  - 或者反过来衡量拟合程度的函数时效用函数(utility function)
  - 通过分析问题，确定问题的损失函数或者效用函数
    - 通过最优化损失函数或者效用函数，获得机器学习的模型
  - 近乎所有参数学习算法都是这样的套路
    - 最优化原理，凸优化

## 最小二乘法的向量化运算

- 直接套用公式计算最小二乘法的效率比较低
  - 使用向量化运算
  - 最小二乘法的分子和分母都符合向量中相应项相乘在相加
    - 符合向量点乘的运算


## 回归算法的评价

- 回归算法的目的
  - 使训练数据集的损失函数最小，效用函数最大
  - 用测试数据集进行评价
  - 但是如果直接使用预测值和真值之间的差距的平方和，会受到数据数量的影响
    - 所以除掉数据个数：均方误差MSE(Mean Squared Error)
  - 还有量纲的问题
    - 单位平方不一定存在意义，所以对MSE进行开方
    - 均方根误差RMSE(Root Mean Squared Error)
  - 平均绝对误差MAE(Mean Absolute Error)
  - 使用最小二乘法的话训练的时候就是将MSE最小化的过程

- 但是RMSE和MAE还是存在问题，不同数据无法直接进行比较
  - 使用R Squared
  - 1- (Residual Sum of Squares) / (Total Sum of Squares)
    - Residual Sum of Squares: 使用模型预测产生的错误
    - Total Sum of Squares：使用Baseline模型(y = y bar)预测产生的错误
    - 相当于评价了使用模型没有产生错误的指标

## R Squared

- R^2 <= 1
  - 越大越好，不犯任何错误就是R^2 = 1
  - 模型等于基准模型时R^2 = 0
  - 如果R^2 < 0说明还不如基准模型，说明数据不存在任何**线性关系**
  - 1 - (MSE/variance)


## 多元线性回归

- 多元线性回归的正规方程解(Normal Equation)
  - 时间复杂度高:O(n^3)，可优化至(O(n^2.4))
  - 优点：不需要对数据做归一化处理

## 线性回归算法总结

- 只能解决回归问题
  - 但是是很多分类方法的基础
- 对数据有假设：线性
  - kNN对数据没有假设
  - 但是如果有线性关系的话用线性回归好很多
- 具有强解释性，是白盒子算法