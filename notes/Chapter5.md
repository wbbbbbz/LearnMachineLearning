# 第 5 章 线性回归法

- 解决回归问题
  - 思想简单，实现容易
  - 许多强大的非线性模型的基础
  - 结果具有很好的可解释性
  - 蕴含机器学习中的很多重要思想

- 寻找一条直线，最大程度的拟合样本特征和样本输出标记之间的关系
  - 横轴是样本特征，纵轴是样本输出标记
  - 因为输出是数值
  - 样本特征只有一个，成为简单线性回归
  - y=ax+b

- 希望预测值和真值之间的差距尽量小
  - 差距的平方和尽可能小的a和b
  - 最小二乘法

- 一类机器学习算法的基本思路
  - 找到参数使得预测值和真值之间的差距尽量小
  - 预测值和真值之间的差距可以使用损失函数(loss function)
  - 或者反过来衡量拟合程度的函数时效用函数(utility function)
  - 通过分析问题，确定问题的损失函数或者效用函数
    - 通过最优化损失函数或者效用函数，获得机器学习的模型
  - 近乎所有参数学习算法都是这样的套路
    - 最优化原理，凸优化

## 最小二乘法的向量化运算

- 直接套用公式计算最小二乘法的效率比较低
  - 使用向量化运算
  - 最小二乘法的分子和分母都符合向量中相应项相乘在相加
    - 符合向量点乘的运算


## 回归算法的评价

- 回归算法的目的
  - 使训练数据集的损失函数最小，效用函数最大
  - 用测试数据集进行评价
  - 但是如果直接使用预测值和真值之间的差距的平方和，会受到数据数量的影响
    - 所以除掉数据个数：均方误差MSE(Mean Squared Error)
  - 还有量纲的问题
    - 单位平方不一定存在意义，所以对MSE进行开方
    - 均方根误差RMSE(Root Mean Squared Error)
  - 平均绝对误差MAE(Mean Absolute Error)
  - 使用最小二乘法的话训练的时候就是将MSE最小化的过程