# 第 9 章 逻辑回归

- 逻辑回归(Logistic Regression)
  - 解决分类问题
  - 将样本的特征和样本发生的概率联系起来。因为概率是一个数，所以称为回归
  - 回归的本质是yhat = f(x)，yhat就是要求的目标
  - 而逻辑回归的本质是phat = f(x)，然后根据预测的概率进行分类
    - yhat = 1 when phat >= 0.5等等
  - 可以看作回归算法(不经过最后的分类步骤，只计算根据特征计算的概率)，也可看作分类算法
  - 一般作为分类算法，只可以解决二分类问题
  - 概率的值域为[0, 1]，所以不能直接使用线性回归(没有值域)

- 解决方案很简单，先进行普通的线性回归，然后将得到的预测值再带入sigma函数，求出概率
  - phat = σ(θT . xb)
  - sigma函数一般使用Sigmoid函数
    - 1 / (1 + e ^ -t)
  - phat = 1 / (1 + e ^ - (θT . xb))
  
- 问题：对于给定的样本数据集X, y，如何找到参数θ使得这样的方式可以最大程度获得样本数据集X对应的分类输出y

## Sigmoid函数

- 值域就是(0, 1)
  - t>0时p>0.5
  - t<0时p<0.5

## 定义损失函数

- 如果真值为1
  - 则p越小，cost越大
- 如果真值为0
  - 则p越大，cost越大
- 所以定义一个损失函数
  - 当y=1,cost = -log(phat)
  - 当y=0,cost = -log(1-phat)

- 但是使用分类函数不太好，所以可以合成一个函数
- cost = -ylog(phat)-(1-y)log(1-phat)
  - 因为y只有1和0的区别，y的取值会导致有一项会消失
- cost函数需要求平均
- 这个函数没有公式直接计算θ
- 但是可以使用梯度下降法求解，这个函数时凸函数，存在全局唯一最优解
- 梯度可以数学求解的 
  - 其实与线性回归的解非常相似，只不过Xbθ变成σ(Xbθ)
  - 因为σ(Xbθ)才是逻辑回归中相应的yhat


## 逻辑回归的θ与决策边界

- θT.dot(xb)=0称为决策边界
  - >=0时分类为1，<0时分类为0
- 这个式子是两个向量点乘
  - 如果X有两个特征，θT.dot(xb)=θ0+θ1x1+θ2x2=0
  - 是一根关于x1和x2的直线
  - 将上述式子绘图后可以发现是决策边界，大体将两部分数据分开
  - 如果数据处于决策边界，分成任何一类都可
- 不规则的决策边界(比如多项式分类情况)的绘制方法
  - 在一个空间中计算所有点的分类情况，然后进行绘制即可


## 非线性的逻辑回归

- 分类并不一定是以一条直线分界的，比如存在圆形边界等等
- 这个时候就需要引入多项式项
- 添加多项式项之后容易因为degree太高发生过拟合
- sklearn推荐使用正则化！

## 逻辑回归中使用正则化

- 第8章中学习的正则化的超参数α是在L1或L2正则化之前的
- 还有一种是在J(θ)项前加上C，大C超参数。
  - 和α的倒数其实是相同意义
  - 但是大C更加常用，是sklearn使用的方式
  - 因为在损失函数前的超参数导致必须进行正则化，正则项系数不可为0