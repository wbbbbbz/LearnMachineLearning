# 第 4 章 最基础的分类算法-k近邻算法 kNN

- kNN算法(k Nearest Neighbor)
  - 思想简单，效果好
  - 适合入门，可以解释细节问题

- 在特征空间中已经存在一部分数据和其对应的分类
  - 此时来了一个新的数据，如何对其进行分类？

- 假设该模型已经得到一个k值
  - 那就寻找离新的点最近的k个点，让这几个点进行投票
  - 比如投票结果是k:0，那么就有很大概率是第一分类
  - 本质是认为两个样本足够相似则有更高概率属于同一个类别
    - 只看一个是不够的，应该看k个样本，越多则越相似

- 可解决分类，也可以解决回归问题

- 距离可以使用欧拉距离

- 输入大量学习资料(训练数据集，比如X_train,y_train)->机器学习算法(进行fit,拟合)->模型->输出结果
  - 输入样例->模型(进行predict)->输出结果
- kNN是几乎唯一不需要训练过程的算法
  - 可以认为没有模型，也可以认为训练数据集就是模型本身(为了和其他算法保持一致)

- Scikit-learn中的算法都是进行fit，然后得到模型再predict
  - 只不过kNN的fit就非常简单

- 训练得到的模型不能直接在真实环境中使用
  - 模型如果很差怎么办
  - 真实环境难以获得真实的label

## 训练和测试数据集的分离

- 改进这个的最好方法是训练和测试数据集的分离
  - 从原始数据中的70%-80%进行训练
  - 训练数据得到模型之后，用测试数据进行测试，判断性能
  - 通过测试数据直接判断模型好坏，在模型进入真实环境前改进模型
  - train test split

