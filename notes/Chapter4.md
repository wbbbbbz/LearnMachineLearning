# 第 4 章 最基础的分类算法-k近邻算法 kNN

- kNN算法(k Nearest Neighbor)
  - 思想简单，效果好
  - 适合入门，可以解释细节问题

- 在特征空间中已经存在一部分数据和其对应的分类
  - 此时来了一个新的数据，如何对其进行分类？

- 假设该模型已经得到一个k值
  - 那就寻找离新的点最近的k个点，让这几个点进行投票
  - 比如投票结果是k:0，那么就有很大概率是第一分类
  - 本质是认为两个样本足够相似则有更高概率属于同一个类别
    - 只看一个是不够的，应该看k个样本，越多则越相似

- 可解决分类，也可以解决回归问题

- 距离可以使用欧拉距离

- 输入大量学习资料(训练数据集，比如X_train,y_train)->机器学习算法(进行fit,拟合)->模型->输出结果
  - 输入样例->模型(进行predict)->输出结果
- kNN是几乎唯一不需要训练过程的算法
  - 可以认为没有模型，也可以认为训练数据集就是模型本身(为了和其他算法保持一致)

- Scikit-learn中的算法都是进行fit，然后得到模型再predict
  - 只不过kNN的fit就非常简单

- 训练得到的模型不能直接在真实环境中使用
  - 模型如果很差怎么办
  - 真实环境难以获得真实的label

## 训练和测试数据集的分离

- 改进这个的最好方法是训练和测试数据集的分离
  - 从原始数据中的70%-80%进行训练
  - 训练数据得到模型之后，用测试数据进行测试，判断性能
  - 通过测试数据直接判断模型好坏，在模型进入真实环境前改进模型
  - train test split

## 超参数和模型参数

- 超参数：在算法运行前需要决定的参数
  - kNN算法中的k是典型的超参数
- 模型参数：算法过程中学习的参数
  - kNN算法没有模型参数

- 寻找好的超参数
  - 领域知识
  - 经验数值
  - 实验搜索
    - 尝试测试几组，找出最好的
    - 如果找到的参数在范围界限附近，那么最好在该数字附近再进行一下搜索
    - 因为一般是连续的

## kNN模型中的距离

- 近邻节点是有距离的。
  - 一般权重是距离的倒数
  - 考虑了距离的话能解决平票问题
  - 超参数(考虑距离与否)

- 除了欧拉距离，还有曼哈顿距离，明可夫斯基距离(Minkowski Distance)
  - 曼哈顿距离：两点间所有维度距离之和(绝对值距离)
  - 明可夫斯基距离: p = 1是曼哈顿距离，p=2是欧拉距离
  - 超参数p

- 更多的距离定义
  - 向量空间余弦相似度Cosine Similarity
  - 调整余弦相似度Adjusted Cosine Similarity
  - 皮尔森相关系数Pearson Correlation Coefficient
  - Jaccard相似系数Jaccard Coefficient
  - 在scikit learn中的metric可以进行定义

- 网格搜索
  - 对于k, p的每一个点进行搜索
  - 而且超参数之间还有依赖关系，比如考虑距离与否和明可夫斯基距离有依赖关系

## 数据归一化

- 欧拉距离会被数据的绝对大小所影响

- 归一化
  - 映射到0-1之间。最值归一化normalization
    - 适用于分布有明显边界的情况。如果没有明显边界，会受outlier影响较大
  - 均值方差归一化standardization。
    - 把所有数据归一到均值为0方差为1的分布中
      - 数据分布没有明显的边界，有可能存在极端数据值
      - 基本上使用这种方式即可

- 在进行train_test——split的时候进行归一化
  - 测试数据集的归一化要使用meat_train和std_train
  - 测试数据是模拟真实环境
    - 真实环境无法得到数据的均值和方差
    - 对数据的归一化也是算法的一部分
  - 所以要保存训练数据集得到的均值和方差
- sklearn中使用scalar类进行归一化
  - 将训练集传入scalar，然后进行fit，保存关键信息
  - 输入样例之后，进行transform