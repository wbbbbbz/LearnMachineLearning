# 第 11 章 支撑向量机SVM

## 支撑向量机Support Vector Machine

- 逻辑回归算法计算出了决策边界进行分类
- 求出决策边界时通过sigmoid的损失函数最小化计算的，而损失函数是由训练数据集进行支撑的
  - 但是决策边界存在不适定问题，也就是决策边界不一定只存在一条
  - 求出的决策边界对于没有看见的测试数据集能否有好的泛化能力？

- 比较好的决策边界
  - 不仅能将训练数据集样本进行很好的划分，还要考虑泛化能力
  - 找到一个决策边界，使最近的点的距离尽可能最远

- SVM考虑泛化能力没有寄望在数据预处理，正则化
  - 泛化能力考量直接放在算法内部
  - 决策边界要离分类样本尽可能远
  - 有数学理论，统计理论支撑

## 最优的决策边界

- 离决策边界最近的点又可以组成新的决策边界，这些新的决策边界会组成一块区域
  - 这块区域中没有数据
  - SVM尝试寻找一个最优的决策边界，距离两个类别的最近的样本最远
  - 最近的点称为**支撑向量**
  - 最后找到的决策边界可以说是被支撑向量定义的
  - 不同分类的支撑向量的边界之间的距离称为margin
  - margin = 2d，d是支撑向量到最优决策的距离

- 上述讨论的是线性可分问题，Hard Margin SVM
  - 要求一定要有线性(或者高维的超平面)的最优决策边界

- 线性不可分问题，Soft Margin SVM

## 数学表达margin

- [推导 | SVM详解（1）SVM基本型 - 知乎](https://zhuanlan.zhihu.com/p/35755150)

- margin = 2d，SVM要最大化d
- 解析集合，点到直线的距离
  - (x, y)到Ax+By+C=0的距离
  - [点到直线距离公式_高三数学解析几何](https://dayi.jd100.com/question/1066206/)
  - 拓展n维空间
    - θT.dot(xb)=0表示为wT.dot(x) + b = 0
    - w是weight，分理出截距出来即可
    - 所以距离应该是|wT.dot(x) + b| / ||w||
    - ||w||=sqrt(w1^2+w2^2...)
  - 假设决策边界是wT.dot(x) + b，求支撑向量到边界都最大的情况

- 定义y的分类是1和-1，计算方便

- 经过数学推导之后，最优化函数会变成min(1/2||w||^2)

- 有条件的最优化问题

## Soft Margin和SVM的正则化

- Hard margin SVM要求决策边界必须可以进行分类，如果出现outlier时Hard margin SVM求得的解泛化能力低
- 对hard的式子中加入宽松参数eta，使得SVM可以犯错，数据点可以在支撑向量构成的空间内部
- eta是对每一个数据点都存在的参数，但是容错空间不能太大
  - 所以需要加入正则化，也就是eta的和。并且eta>=0
  - min1\2||w||^2 + C sigma(eta)
  - C也是一个超参数
  - 加入L1正则，保证更高的容错能力
  - L2正则也可以
  - C如果越大，那么损失函数中eta所占比例越大，所以优化是需要减小eta，也就是减小容错空间
## SVM之前的标准化处理

- SVM算法涉及距离，所以使用之前必须使用标准化。


## SVM多项式特征

- 可以直接增加多项式
- 或者可以直接使用多项式核函数