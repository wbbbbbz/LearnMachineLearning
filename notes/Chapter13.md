# 第 13 章 集成学习和随机森林

## 集成学习

- 多个算法进行计算，然后综合投票，集成决策
- sklearn中有voting classifer

## hard和soft voting classifer

- hard: 少数服从多数
- soft: 更合理的投票应该有权值
  - 将每一个模型分类的概率作为权值

- soft voting就要求集合每一个模型都能估计概率
- 比如逻辑回归算法计算predict_proba
  - 有这个方法就支持soft voting

- kNN也支持概率
- 决策树也支持概率
  - 因为叶子节点不一定是一个类。概率就是叶子节点中占比最高的分类占整个叶子节点样本数的比例

- SVM的SVC看似不支持概率。但是可以通过消耗一定的计算资源计算出概率
  - probability参数设置为true即可算出概率

## 集成学习的局限

- 虽然有很多机器学习方法，但是从投票的角度看，仍然不够多
  - 所以要创建子模型，集成更多子模型的意见
  - 子模型之间不能一致，子模型之间要有差异性

- 如何创建差异性？
  - 每个子模型只看样本数据的一部分
  - 每个子模型不需要太高的准确率(因为训练数据集下降了)
    - 当然还是需要比平均水平高
  - 随着子模型数量增多，集成学习的整体准确率会增高
    - 所以集成学习威力巨大，很多场景都会使用

- 集成非参数学习模型，更能产生差异比较大的子模型，有很多参数的不同
  - 成百上千的子模型的时候，首选就是决策树(参数，剪枝等等很多参数不同)

### Bagging 和 Pasting

- 每个子模型只看样本数据的一部分
  - 取样：放回取样Bagging，不放回取样Pasting
  - 一般使用Bagging
  - Bagging不受随机影响
  - 统计学中放回取样：bootstrap

