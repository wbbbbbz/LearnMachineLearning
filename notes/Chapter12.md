# 第 12 章 决策树

## 什么是决策树

- 如果每一个节点都是Yes或No就很好分类
- 如果是数字特征，则可以使用范围进行判断
  - x1<2.4
    - x2<1.8
      - ....
- 非参数学习算法
- 可以解决分类问题，天然可以解决多分类问题
- 也可以解决回归问题
- 具有非常好的可解释性

## 如何构建决策树

- 每个节点在那个维度做划分？
- 某个维度在哪个值上进行划分？
- 划分后使得信息熵降低
- 解决方法：对所有划分方式搜索一次就好
  - 在根节点(拥有全部数据的节点)找到一个维度进行划分，希望整体信息熵降低
  - 然后再对划分出来的两个节点继续划分，使整体信息熵降低
  - 不断递归

## 信息熵

- 信息论的基本概念
- 熵在信息论中代表随机变量不确定度的度量
- 熵越大，数据的不确定性越高
- 熵越小，数据的不确定性越低

- 香农提出的信息熵计算公式
  - H = -sigma(pi \* log(pi))
  - pi是一个系统的k类信息中某种信息所占的比例
  - pi小于1，log(pi)<0，所以H一定大于0
  - 比如一个系统中某一个信息比例特别大，就会发现算出的信息熵趋向为0，不确定性就会很低

## 基尼系数

- 也可以通过基尼系数进行划分
- G = 1 - sigma(pi^2)
- 基尼系数越小，不确定性越弱
- 基尼系数越大，不确定性越强

## 信息熵vs基尼系数

- 信息熵的计算比基尼系数的计算稍慢
  - 信息熵中需要计算log，非线性函数
- sklearn中默认为基尼系数
- 大多数时候二者没有特别的效果优劣
- 


